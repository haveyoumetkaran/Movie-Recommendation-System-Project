# -*- coding: utf-8 -*-
"""Major_Project_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cR99KQZ1cpOQbmbzqPErklavtztqAYmW
"""

pip install -U sentence-transformers

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import random
import warnings
warnings.filterwarnings('ignore')
import os
import random
import operator
import requests
from scipy import sparse
import sys
# from surprise import Dataset, Reader
# from surprise import KNNBasic, SVD
# from surprise.model_selection import train_test_split
# from surprise import accuracy
# from surprise.dataset import DatasetAutoFolds
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

df_ratings = pd.read_csv('/content/drive/MyDrive/Major_Project_ML/ratings.csv')
df_links = pd.read_csv('/content/drive/MyDrive/Major_Project_ML/links.csv')
df_movies = pd.read_csv('/content/drive/MyDrive/Major_Project_ML/movies.csv')
df_tags = pd.read_csv('/content/drive/MyDrive/Major_Project_ML/tags.csv')

"""# Preprocessing and Visualization"""

df_movies.head()

df_ratings.head()

df_links.head()

df_tags.head()

dff_tags = df_tags.copy(deep=True)

dff_tags['timestamp'] = pd.to_datetime(dff_tags['timestamp'],unit='s') # Converts timestamp into Date and Time format
dff_tags['date'] = dff_tags['timestamp'].dt.date
dff_tags['time'] = dff_tags['timestamp'].dt.time

dff_tags['year'] = dff_tags['timestamp'].dt.year
dff_tags['month'] = dff_tags['timestamp'].dt.month
dff_tags['date'] = dff_tags['timestamp'].dt.date
dff_tags['hour'] = dff_tags['timestamp'].dt.hour

dff_tags

dff_tags.isnull().sum()

dff_tags.dropna(inplace=True)

dff_tags.isnull().sum()

#Extracted the date , time, year , month, hour
#Now, binning

years = list(dff_tags['year'].unique())
print(years)

len(years)

#Code to find no. of tags for every year
year_count=[]
for i in range(len(years)):
  dp = dff_tags[dff_tags['year']==years[i]]
  year_count.append(len(dp))

yearss = dict(zip(years,year_count))

yearss

dff_tags['month'].unique()

#Code to bin years
bins_years = [2004,2008,2011,2014,2017]
#Bins are such that [2005,2008) , [2008,2011)
labels_years = ['2005-2007','2008-2010','2011-2013','2014-2016']
dff_tags['year_bins'] = pd.cut(dff_tags['year'],bins=bins_years,labels=labels_years)

dff_tags

#Code to bin months
bins_month = [0,3,6,9,13]
labels_month = ['Jan-Mar','Apr-Jun','Jul-Sep','Oct-Dec']
dff_tags['month_bins'] = pd.cut(dff_tags['month'],bins=bins_month,labels=labels_month)

dff_tags

#Code to bin hours
bins_hour = [-1,4,8,12,16,20,24]
labels_hour = ['00:00-03:59','4:00-7:59','8:00-11:59','12:00-15:59','16:00-19:59','20:00-23:59']
dff_tags['hour_bins'] = pd.cut(dff_tags['hour'],bins=bins_hour,labels=labels_hour)

dff_tags

#Pie plot between no. of tags and Year bins

year_counts = dff_tags['year_bins'].value_counts()
plt.pie(year_counts,labels=year_counts.index,autopct='%1.1f%%')
plt.title('Year Distribution')
plt.show()

#Pie plot between no. of tags and Month bins

month_counts = dff_tags['month_bins'].value_counts()
plt.pie(month_counts,labels=month_counts.index,autopct='%1.1f%%')
plt.title('Month Distribution')
plt.show()

#Pie plot between no. of tags and Hour bins

hour_counts = dff_tags['hour_bins'].value_counts()
plt.pie(hour_counts,labels=hour_counts.index,autopct='%1.1f%%')
plt.title('Distribution as per Hour of the Day')
plt.show()

# Create a bar chart of the tag frequency
tag_freq = dff_tags['tag'].value_counts()
tag_freq[:10].plot(kind='bar')
plt.xlabel('Tag')
plt.ylabel('Frequency')
plt.title('Top 10 Most Common Tags')
plt.show()

#Now we will append Movie names as per their titles in dff_tags

dff_tags = pd.merge(dff_tags, df_movies[['movieId', 'title']], on='movieId', how='left')

dff_tags

dff_tags["year_bins"].value_counts()

movieid_years = dff_tags.copy(deep=True)
userid_years = dff_tags.copy(deep=True)

#Code to drop following columns from dataset
movieid_years.drop(['userId','timestamp','date','time','year','month','hour','month_bins','hour_bins','title'],axis=1,inplace=True)
userid_years.drop(['movieId','timestamp','date','time','year','month','hour','month_bins','hour_bins','title'],axis=1,inplace=True)

movieid_years

userid_years

movieid_years

df_movies

#Code to find following unique values

df_movies_columns = list(df_movies.columns)
for i in range(len(df_movies_columns)):
  print("Unique values in",df_movies_columns[i],len(dict(df_movies[df_movies_columns[i]].value_counts())))

df_movies.isnull().sum()

# Code to print duplicates in df_movies dataset

all_movies_count = dict(df_movies["title"].value_counts())
# print(all_movies_count)
print("Movies with 2 occurrence in dataset-")
for i in all_movies_count:
  if(all_movies_count[i]>1):
    print(i,"with tags-")
    print(df_movies[df_movies["title"]==i])
    print()

# Code to split genres over '|'

new_dataset = dict()
for i in range(len(df_movies)):
  if(df_movies["title"].iloc[i] in new_dataset.keys()):
    new_dataset[df_movies["title"].iloc[i]]= (new_dataset[df_movies["title"].iloc[i]] + "|" +str(df_movies["genres"].iloc[i]))
  else:
    new_dataset[df_movies["title"].iloc[i]]=str(df_movies["genres"].iloc[i])
for i in new_dataset:
  temp = new_dataset[i].split("|")
  new_dataset[i] = list(set(temp))

#Code to create a list of genres for each movie

all_generes_splited = []
for i in range(len(df_movies)):
  all_generes_splited.append(new_dataset[df_movies["title"].iloc[i]])

df_movies["genres_list"] = all_generes_splited
df_movies

#Code to calculate no. of movies containing given genres

all_generes = dict()
for i in range(len(df_movies)):
  temp_list = list(df_movies["genres_list"].iloc[i])
  for j in range(len(temp_list)):
    if(temp_list[j] in list(all_generes.keys())):
      all_generes[temp_list[j]] += 1
    else:
      all_generes[temp_list[j]] = 1

all_generes

#Code to plot donut chart showing distribution of genres in the dataset

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager
plt.rcdefaults()
plt.rcParams['font.family'] = 'DeJavu Serif'
plt.rcParams['font.serif'] = ['Times New Roman']
labels = all_generes.keys()
sizes = all_generes.values()

fig, ax = plt.subplots(figsize=(8, 12), subplot_kw=dict(aspect="equal"))

wedges, texts = ax.pie(sizes, wedgeprops=dict(width=0.5), startangle=-40)

kw = dict(arrowprops=dict(arrowstyle="->"), va="center")
for p, label in zip(wedges, labels):
    ang = np.deg2rad((p.theta1 + p.theta2)/2)
    y = np.sin(ang)
    x = np.cos(ang)
    horizontalalignment = "center" if abs(x) < abs(y) else "right" if x < 0 else "left"
    ax.annotate(label, xy=(0.75*x, 0.75*y), xytext=(1.3*x, 1.3*y),
                horizontalalignment=horizontalalignment, **kw)
plt.tight_layout()
plt.title("Pie chart showing distribution of different genres in dataset")
plt.show()

#Code to store genres for all movies

all_generes_list = list(all_generes.keys())
all_geners_list_for_all_rows = []
for i in range(len(df_movies)):
  temp_list = list(np.zeros(len(all_generes_list),dtype=int))
  for j in range(len(df_movies["genres_list"].iloc[i])):
    temp_list[all_generes_list.index(list(df_movies["genres_list"].iloc[i])[j])] = int(1)
  all_geners_list_for_all_rows.append(temp_list)

all_geners_split_dataframe = pd.DataFrame(all_geners_list_for_all_rows,columns=all_generes_list)

#Creates new dataframe that contains movies with their genres

df_movies_new = pd.concat([df_movies,all_geners_split_dataframe],axis=1)
df_movies_new.drop(["genres"],axis=1,inplace=True)
df_movies_new

import matplotlib.pyplot as plt
df_movies_new["movieId"].value_counts()

copy_ratings = df_ratings.copy(deep = True)
copy_movies = df_movies.copy(deep = True)
copy_links = df_links.copy(deep = True)
copy_tags = df_tags.copy(deep = True)

#Code to store ratings for each movie

movie_rating = {}
id_list = copy_ratings['movieId'].to_numpy()
ratings_list = copy_ratings['rating'].to_numpy()

for i in range(0,len(copy_ratings),1):
  if id_list[i] in movie_rating.keys():
    movie_rating[id_list[i]].append(ratings_list[i])
  else:
    movie_rating[id_list[i]] = []
    movie_rating[id_list[i]].append(ratings_list[i])

key_list = list(movie_rating.keys())
key_list.sort()
print(type(key_list))
print(key_list)

#Code to find IDs of movies without ratings

link_id = copy_links['movieId'].to_numpy()

missing_id = []

for i in link_id:
  if i not in key_list:
    missing_id.append(i)
print(missing_id)

for i in range(0,len(missing_id),1):
  movie_rating[missing_id[i]] = []
  movie_rating[missing_id[i]].append(0.1)

final_key_list = list(movie_rating.keys())
final_key_list.sort()
print(final_key_list)
print(len(final_key_list))

movie_rating_df = pd.DataFrame()

movie_rating_df['movieId'] = final_key_list
movie_rating_df

print(np.mean(movie_rating[key_list[1]]))

#Code to find average rating for each movie

mean_list = []

for i in range(0,len(final_key_list),1):
  temp_mean = round(np.mean(movie_rating[final_key_list[i]]),3)
  mean_list.append(temp_mean)

movie_rating_df['avg_rating'] = mean_list
movie_rating_df

#Code to find no. of ratings per movie

number_list = []

for i in range(0,len(final_key_list),1):
  number_list.append(len(movie_rating[final_key_list[i]]))

movie_rating_df['Rating_no'] = number_list
movie_rating_df

m = np.percentile(number_list,90,axis = 0)
m

#Code to find weighted ratings for each movie using IMBd's formula for weighted ratings

movie_weight_rating = []
movie_mean = np.mean(mean_list)
m=1000
for i in range(0,len(final_key_list),1):
  num = number_list[i]*mean_list[i] + m*movie_mean
  den = number_list[i] + m
  temp = round(num/den,3)
  movie_weight_rating.append(temp)

movie_rating_df['Weighted rating'] = movie_weight_rating
movie_rating_df

#Code to store ratings given by every user

user_rating = {}
userid_list = copy_ratings['userId'].to_numpy()
ratings_list = copy_ratings['rating'].to_numpy()

for i in range(0,len(copy_ratings),1):
  if userid_list[i] in user_rating.keys():
    user_rating[userid_list[i]].append(ratings_list[i])
  else:
    user_rating[userid_list[i]] = []
    user_rating[userid_list[i]].append(ratings_list[i])

user_key_list = list(user_rating.keys())
user_key_list.sort()
print(type(user_key_list))
print(user_key_list)

#Code to store average ratings for each person and no. of ratings given by him

user_rating_df = pd.DataFrame()

user_rating_df['userId'] = user_key_list

mean_list_2 = []
number_list_2 = []

for i in range(0,len(user_key_list),1):
  temp_mean = round(np.mean(user_rating[user_key_list[i]]),3)
  mean_list_2.append(temp_mean)
  number_list_2.append(len(user_rating[user_key_list[i]]))

user_rating_df['avg_rating'] = mean_list_2
user_rating_df['Rating_no'] = number_list_2

user_rating_df

m = np.percentile(number_list_2,90,axis = 0)
m

#Code to find weighted ratings for every user

user_weight_rating = []
user_mean = np.mean(mean_list_2)
m = 227
for i in range(0,len(number_list_2),1):
  num = number_list_2[i]*mean_list_2[i] + m*user_mean
  den = number_list_2[i] + m
  temp = round(num/den,3)
  user_weight_rating.append(temp)

user_rating_df['Weighted rating'] = user_weight_rating
user_rating_df

#Code to store tags for each movie

movie_tags = {}
tag_list = copy_tags['tag'].to_numpy()
tag_movie_id = copy_tags['movieId'].to_numpy()

for i in range(0,len(copy_tags),1):
  if tag_movie_id[i] in movie_tags.keys():
    movie_tags[tag_movie_id[i]].append(tag_list[i])
  else:
    movie_tags[tag_movie_id[i]] = []
    movie_tags[tag_movie_id[i]].append(tag_list[i])

tag_keys = list(movie_tags.keys())

#Code to remove duplicate tags

for i in range(0,len(tag_keys),1):
  movie_tags[tag_keys[i]] = list(set(movie_tags[tag_keys[i]]))

#Code to calculate no. of tags for each movie and sort dictionary according to movieId

tag_keys.sort()
len_list = []
tags_for_movie = []

for i in range(0,len(tag_keys),1):
  len_list.append(len(movie_tags[tag_keys[i]]))
  tags_for_movie.append(movie_tags[tag_keys[i]])

print(len_list)

movie_tags_df = pd.DataFrame()

movie_tags_df['movieId'] = tag_keys
movie_tags_df['tags'] = tags_for_movie
movie_tags_df['Tag no.'] = len_list

movie_tags_df

#Code to find tags given by each user

user_tags = {}
tag_list = copy_tags['tag'].to_numpy()
tag_user_id = copy_tags['userId'].to_numpy()

for i in range(0,len(copy_tags),1):
  if tag_user_id[i] in user_tags.keys():
    user_tags[tag_user_id[i]].append(tag_list[i])
  else:
    user_tags[tag_user_id[i]] = []
    user_tags[tag_user_id[i]].append(tag_list[i])

user_tag_keys = list(user_tags.keys())
user_tag_keys.sort()

for i in range(0,len(user_tag_keys),1):
  user_tags[user_tag_keys[i]] = list(set(user_tags[user_tag_keys[i]]))

#Code to calculate no. of tags for each user and sort dictionary according to userId

len_list_user = []
tags_for_user = []

for i in range(0,len(user_tag_keys),1):
  len_list_user.append(len(user_tags[user_tag_keys[i]]))
  tags_for_user.append(user_tags[user_tag_keys[i]])

print(len_list_user)

user_tags_df = pd.DataFrame()

user_tags_df['userId'] = user_tag_keys
user_tags_df['tags'] = tags_for_user
user_tags_df['Tag no.'] = len_list_user

user_tags_df

l = len(movie_rating_df.columns)-1


train_df = movie_rating_df.sample(frac = 0.65)
test_df = movie_rating_df.drop(train_df.index)

movie_train_x,movie_train_y = train_df.iloc[:,:l],train_df.iloc[:,l]
movie_test_x,movie_test_y = test_df.iloc[:,:l],test_df.iloc[:,l]

from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score as acc
from sklearn.metrics import mean_squared_error as mse

model_xgb = XGBRegressor()

model_xgb.fit(movie_train_x,movie_train_y)

pred_y = model_xgb.predict(movie_test_x)

err = mse(movie_test_y,pred_y)

print("The mean squared error is :",err)
print("The accuracy is :",100-err)

print(pred_y)

df_movies_new["Weighted rating"] = movie_rating_df["Weighted rating"]
df_movies_new["Rating_no"] = movie_rating_df["Rating_no"]
df_movies_new

df_movies_new.sort_values(by="Weighted rating",inplace = True,ascending=False)

#Function to retrieve year from movie name

def get_year(inp:str):
  out = ""
  is_start = 0
  hasFound = False
  for i in range(len(inp)):
    if(inp[i]=="(" and inp[i+5]==")"):
      is_start = i
      hasFound = True
  if(hasFound):
    out = inp[is_start+1:is_start+5]
  else:
    out = 0
  return int(out)

#Code to store year for every movie

all_years_movies = []
for i in range(len(df_movies_new)):
  all_years_movies.append(get_year(df_movies_new["title"].iloc[i]))

df_movies_new["year"] = all_years_movies

# df_movies_new["Weighted rating"] = movie_rating_df["Weighted rating"]
# df_movies_new["Rating_no"] = movie_rating_df["Rating_no"]
df_movies_new

#Function to extract only words from given input

def extract_all_words(inp):
  all_words = []
  word = ""
  for i in range(len(inp)):
    if(inp[i]==" "):
      all_words.append(word)
      word = ""
    elif(inp[i]=="(" or inp[i]==")"):
      break
    else:
      word += inp[i]
  return all_words
# all_movies_names_list = []
# for i in range(len(df_movies_new)):
#   all_movies_names_list.append(extract_all_words(df_movies_new["title"].iloc[i]))
# flatlist=list(set([element for sublist in all_movies_names_list for element in sublist]))
# len(flatlist)

# df_tags.sort_values(by="movieId",inplace = True)
df_tags

# Code to store tags for each movie without ',' or anything else

tags_string_dict = {}
for i in range(len(df_tags)):
  if(str(df_tags["movieId"].iloc[i]) not in tags_string_dict.keys()):
    tags_string_dict[str(df_tags["movieId"].iloc[i])] = str(df_tags["tag"].iloc[i]).lower()
  else:
    if(str(df_tags["tag"].iloc[i]) not in tags_string_dict[str(df_tags["movieId"].iloc[i])].lower()):
      tags_string_dict[str(df_tags["movieId"].iloc[i])] += str(" " + str(df_tags["tag"].iloc[i])).lower()

temp_list = []
for i in range(len(df_movies_new)):
  if(str(df_movies_new["movieId"].iloc[i]) in tags_string_dict.keys()):
    temp_list.append(tags_string_dict[str(df_movies_new["movieId"].iloc[i])])
  else:
    temp_list.append("")

df_movies_new["tags"] = temp_list
df_movies_new

df_movies_new.head()

#Code to plot correlation between genres

df_movies_new
def plot_corr(df):
    plt.subplots(figsize=(20, 15))
    sns.heatmap(df.corr(),annot=True,linewidths=.5,annot_kws={"fontsize":15})
    plt.yticks(rotation=0,fontsize=15)
    plt.xticks(rotation=0,fontsize=15)
    
    plt.show()

plot_corr(df_movies_new[df_movies_new.columns[3:len(df_movies_new.columns)-1]])

#Plot between year of release and movies

temp = list(df_movies_new["year"].unique())
temp.remove(0)
# print(min(temp))
all_genres = list(df_movies_new.columns)[3:len(df_movies_new.columns)-2]
# print(all_genres)
y_all = []
x_ticks = []
for year in range(1920,max(temp) + 10-(max(temp)%10),10):
  y_temp = np.zeros(len(all_genres))
  x_ticks.append(year)
  for i in range(year,year+10):
    df_movies_temp = df_movies_new[df_movies_new["year"]==year]
    for j in range(len(all_genres)):
      y_temp[j] += len(df_movies_temp[df_movies_temp[all_genres[j]]==1])
  y_all.append(y_temp)

import numpy as np
import matplotlib.pyplot as plt
# x_ticks.append(2010)
plt.figure(figsize = (10,10))
plt.stackplot(x_ticks,np.array(y_all).T, labels=all_genres)
plt.legend(loc='upper left')

plt.show()

#copy_ratings.drop(["year","month","hour"],axis = 1,inplace=True)
copy_ratings

print(all_generes_list)

user_rating_df

temp_list = list(user_rating_df["Rating_no"].to_numpy())
temp_id = user_rating_df["userId"].to_numpy()

#Code to return userIds with less than 1000 ratings

print(temp_list)
temp_list.sort(reverse = True)
print(temp_list)
l = len(temp_list)

for i in range(0,len(temp_list),1):
  if(temp_list[i]<1000):
    end_index = i
    break
print(end_index)

small_list = temp_list[0:end_index]

print(small_list)

#Codeto find which Users to keep and which to drop

keep_id = []
drop_id = []
temp = user_rating_df['Rating_no'].to_numpy()
for i in range(0,len(temp_id),1):
  if temp[i] in small_list:
    keep_id.append(i+1)
  else:
    drop_id.append(i+1)

print(keep_id)
print(drop_id)

copy_ratings_np = copy_ratings.to_numpy()
copy_ratings_np

#Code to store information for every userId in a dictionary

df_to_dict = {}
user_ids = copy_ratings['userId'].to_numpy()


for i in range(0,len(copy_ratings),1):
  if user_ids[i] in df_to_dict.keys():
    temp_list = copy_ratings_np[i,:]
    df_to_dict[user_ids[i]].append(temp_list)
  else:
    df_to_dict[user_ids[i]] = []
    temp_list = copy_ratings_np[i,:]
    df_to_dict[user_ids[i]].append(temp_list)

#Code to select only those userIds which are in the keep_id list

neural_list = []

for i in range(0,len(keep_id),1):
  t = keep_id[i]
  for j in range(0,len(df_to_dict[keep_id[i]]),1):
    neural_list.append(df_to_dict[keep_id[i]][j])

neural_df = pd.DataFrame(neural_list, columns = copy_ratings.columns)
neural_df

genre_names = list(df_movies_new.columns[3:])
print(genre_names)

neural_df['count'] = np.zeros((len(neural_df),1))
neural_df['avg_rating'] = np.zeros((len(neural_df),1))
neural_df

for i in range(0,len(genre_names),1):
  temp_list = np.zeros((len(neural_df),1))
  neural_df[genre_names[i]] = temp_list

neural_df

print(all_years_movies)

#Code to store release years, genres, weighted ratings and no. of ratings for each movie

all_years_movies_dict = {}
all_genres_movies_dict = {}
all_movies_wr = {}
all_movies_count = {}

temp_id_list = df_movies_new['movieId'].to_numpy()

for i in range(0,len(all_years_movies),1):
  all_years_movies_dict[temp_id_list[i]] = all_years_movies[i]
  all_genres_movies_dict[temp_id_list[i]] = all_geners_list_for_all_rows[i]
  all_movies_wr[temp_id_list[i]] = movie_weight_rating[i]
  all_movies_count[temp_id_list[i]] = number_list[i]

print(all_years_movies_dict)
print(all_genres_movies_dict)

print(all_movies_wr)
print(all_movies_count)

neural_df

#Code to fill values for each row in neural_df according to our created dictionaries

temp_neural = neural_df.to_numpy()

temp_names = list(neural_df.columns)

for i in range(0,len(temp_names),1):
  if(temp_names[i] == genre_names[0]):
    m = i
    break

for i in range(0,len(temp_neural),1):
  l = len(temp_neural[i])-1
  t = int(temp_neural[i][1])
  temp_neural[i][l-1] = all_years_movies_dict[t]
  temp_neural[i][m-2] = all_movies_count[t]
  temp_neural[i][m-1] = all_movies_wr[t]
  k = 0
  for j in range(m,l,1):
    temp_neural[i][j] = all_genres_movies_dict[t][k]
    k+=1

neural_df = pd.DataFrame(temp_neural,columns = temp_names)
neural_df

neural_df.drop(["year",'timestamp'],axis = 1,inplace = True)
neural_df

neural_df

new_user_df = neural_df.copy(deep = True)

user_data_dict = {}

temp_new_user_np = new_user_df.to_numpy()

for i in range(0,len(temp_new_user_np),1):
  t = int(temp_new_user_np[i][0])
  if t in user_data_dict.keys():
    user_data_dict[t].append(temp_new_user_np[i][1:])
  else:
    user_data_dict[t] = []
    user_data_dict[t].append(temp_new_user_np[i][1:])

new_key_list = list(user_data_dict.keys())

for i in range(0,len(new_key_list),1):
  temp_arr = np.array(user_data_dict[new_key_list[i]])
  temp_arr = temp_arr[temp_arr[:,1].argsort()]
  user_data_dict[new_key_list[i]] = temp_arr[len(temp_arr)-500 :len(temp_arr)]

print(genre_names)

df_movies_new['Weighted rating'] = movie_weight_rating
df_movies_new

"""# Scratch Movie Recommender"""

# importing difflib library and creating scratch function for movie name , the theory which we told 
# in report , the movie recommendations are based on genres specific

import difflib
def extract_all_words(inp):
  all_words = []
  word = ""
  for i in range(len(inp)):
    if(inp[i]==" "):
      all_words.append(word)
      word = ""
    elif(inp[i]=="(" or inp[i]==")"):
      break
    else:
      word += inp[i]
  return all_words

def get_recommendations_moviename(movie1):
  close_matches = difflib.get_close_matches(movie1,df_movies_new["title"].to_list(),5,0.3)
  # print(close_matches)
  if len(close_matches)==0:
    print("Cant find movie in Database")
    return 
  
  genres_list_1 = {}
  # genres_list_2 = {}
  # genres_list_3 = {}
  for j in range(len(df_movies_new)):
    if(df_movies_new["title"].iloc[j] in close_matches):
      # print(df_movies_new["title"].iloc[j])
      temp = list(df_movies_new["genres_list"].iloc[j])
      for i in temp:
        if(i in genres_list_1.keys()):
          genres_list_1[i] +=1
        else:
          genres_list_1[i] =1
    
  genres_list_1 = sorted(genres_list_1.items(), key=lambda x:x[1])
  # print(genres_list_1)
  
  all_genres = []
  no_of_genres_to_choose = 3
  for i in range(len(genres_list_1)-1,max(len(genres_list_1)-1-no_of_genres_to_choose,0),-1):
    all_genres.append(genres_list_1[i][0])
  all_genres = set(all_genres)
  # print(all_genres)
  no_of_common = {}
  for i in range(len(df_movies)):
    cnt = 0
    for k in all_genres:
      if(df_movies_new[k].iloc[i]==1):
        cnt+=1
    if(cnt in no_of_common.keys()):
      no_of_common[cnt].append(df_movies["title"].iloc[i])
    else:
      no_of_common[cnt] = [df_movies["title"].iloc[i]]
  # print(no_of_common)
  # return no_of_common
  print()
  print("Top Movie Reccomendations for movie",movie1)
  max_no = max(list(no_of_common.keys()))
  # print(no_of_common[max_no])
  
  similar_matches = close_matches[0:min(len(close_matches),2)]
  
  all_movie_combined_list = [i for i in similar_matches]
  # print(all_movie_combined_list)
  while len(all_movie_combined_list)<10:
    if max_no in no_of_common.keys():
      for i in no_of_common[max_no]:
        if(len(all_movie_combined_list)<10):
          all_movie_combined_list.append(i)
    max_no -=1
  # print(len(all_movie_combined_list))
  print()
  for i in all_movie_combined_list:
    print(i)
  return 
  
  
get_recommendations_moviename("Cars")
get_recommendations_moviename("Batman")

# A helper function which will help us in extracting all the words in a sentence
def extract_all_words(inp):
  all_words = []
  word = ""
  for i in range(len(inp)):
    if(inp[i]==" "):
      all_words.append(word)
      word = ""
    elif(inp[i]=="(" or inp[i]==")"):
      break
    else:
      word += inp[i]
  return all_words
# all_movies_names_list = []
# for i in range(len(df_movies_new)):
#   all_movies_names_list.append(extract_all_words(df_movies_new["title"].iloc[i]))
# flatlist=list(set([element for sublist in all_movies_names_list for element in sublist]))
# len(flatlist)

# Importing sentence transformers library
from sentence_transformers import SentenceTransformer,util
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

#Using that library to predict similar tags among the whole dataset and then predicting results
import difflib
def extract_all_words(inp):
  all_words = []
  word = ""
  for i in range(len(inp)):
    if(inp[i]==" "):
      all_words.append(word)
      word = ""
    elif(inp[i]=="(" or inp[i]==")"):
      break
    else:
      word += inp[i]
  return all_words

def get_recommendations_tags(tagstring):
  sentences = [tagstring]
  no_of_rows = 500
  for i in range(no_of_rows):
    sentences.append(str(df_movies_new["tags"].iloc[i]))
  sentence_embeddings = model.encode(sentences)
  results = []
  for i in range(1,len(sentences)):
    results.append((i-1,util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[i]).numpy()[0][0]))
  sorted_results = sorted(results,key= lambda x:x[1],reverse=True)
  print("Top 10 Reccomendations for the tags - ",tagstring,"is")
  for i in range(10):
    print(df_movies_new["title"].iloc[sorted_results[i][0]])
  return
  
  
get_recommendations_tags("war action fighting guns")

#Giving genre specific recommendations based on ratings 
def get_recommendations_genres(genre1,genre2):
  df_temp = df_movies_new[df_movies_new[genre1]==1]
  print("Top 10 Movies in genres",genre1,genre2,"according to ratings-")
  print()
  for i in range(min(10,len(df_temp[df_temp[genre2]==1]))):
    print(df_temp[df_temp[genre2]==1]["title"].iloc[i],"with rating",df_temp[df_temp[genre2]==1]["Weighted rating"].iloc[i])

get_recommendations_genres("Crime","Drama")

#Giving genre specific recommendations based on ratings 
def get_recommendations_genre(genre1):
  df_temp = df_movies_new[df_movies_new[genre1]==1]
  print("Movies for you in genre",genre1,"according to ratings-")
  print()
  for i in range(min(10,len(df_temp[df_temp[genre1]==1]))):
    print(df_temp[df_temp[genre1]==1]["title"].iloc[i],"with rating",df_temp[df_temp[genre1]==1]["Weighted rating"].iloc[i])

get_recommendations_genre('Animation')

#Giving user specific recommendations 
def user_predict(user_id,dict1 = user_data_dict):
  user_list = list(dict1.keys())
  if user_id not in user_list:
    print("Sorry, user not found")
  else:
    print()
    temp_data = np.array(dict1[user_id])
    temp_data = temp_data[:,4:]
    genre_count = np.zeros((len(temp_data[0]),1))
    for i in range(0,len(temp_data),1):
      for j in range(0,len(temp_data[i]),1):
        if temp_data[i][j] == 1:
          genre_count[j] += 1
    
    genre_index = genre_count[:,0].argsort()
    fav_genres = genre_index[len(genre_index)-3:len(genre_index)]

    genre_str = ''

    for i in range(0,len(fav_genres),1):
      genre_str += ' ' + genre_names[fav_genres[len(fav_genres)-i-1]] + ','

    print("Your Top 3 favourite genres are",genre_str[0:len(genre_str)-1])
    print()

    for i in range(0,len(fav_genres),1):
      get_recommendations_genre(genre_names[fav_genres[len(fav_genres)-i-1]])
      print()

user_predict(17)

"""#Collaborative Filtering"""

#Till now we were doing very basic recommendations,
# Now we will take into the user history also , like which movies user liked the most and then will recommend those movies only 
df_ratings

df_collab = df_ratings.copy(deep=True)

df_collab.drop(['timestamp'],axis=1,inplace=True)

df_collab

df_movies

col_filt = pd.merge(df_collab,df_movies, on='movieId')

col_filt

col_filt.drop(['genres','movieId'],axis=1,inplace=True)

col_filt

# #Working on a subset of 1000 values
# index_list = []
# for i in range(1000):
  
#   k = random.randint(0,22000000)
#   index_list.append(k)
#   if random.randint(0,1):
#     index_list.append(k)
# colab_subset = col_filt.iloc[index_list]

# colab_subset = pd.concat([col_filt.iloc[:7000,:],col_filt.iloc[110000:117000,:],col_filt.iloc[210000:2170000,:]],axis=0)

colab_subset = col_filt.iloc[100000:400000+200000,:]
len(colab_subset["title"].unique())

colab_table = colab_subset.pivot_table(index='title', columns='userId', values='rating', aggfunc='mean')

colab_table

113*194367

colab_table.isnull().sum().sum()

colab_table1 = colab_table.copy(deep=True)

colab_table1.fillna(0,inplace=True)

colab_table1

zero_indices = np.argwhere(np.array(colab_table1)==0)

#indices of null elements:-
print("No. of null elements is :- ",len(zero_indices))
print('\nThe indices of null elements is:-\n',zero_indices)

len(colab_table1)

#Application of K-Nearest Neighbours

knn_colab = NearestNeighbors(metric='cosine', algorithm='brute')
knn_colab.fit(colab_table1.values)
dist_colab, indices_colab = knn_colab.kneighbors(colab_table1.values,n_neighbors=5)

#Every ith row gives the indices of the 10 nearest neighbours of the ith movie
indices_colab

#Corresponding distances
dist_colab

def collab_filtering(df,movie):
  # get the index for movie
  index_for_movie = df.index.tolist().index(movie)
  # find the indices for the similar movies
  sim_movies_ind = indices_colab[index_for_movie].tolist()#sim_movies
  # distances between movie and the similar movies
  sim_movies_dist = dist_colab[index_for_movie].tolist()##movie_distances
  # the position of movie in the list sim_movies
  id_movie = sim_movies_ind.index(index_for_movie)
  # remove movie from the list sim_movies_ind
  sim_movies_ind.remove(index_for_movie)
  # remove movie from the list sim_movie_dist
  sim_movies_dist.pop(id_movie)
  
  # sorted_idx = np.argsort(np.array(sim_movies_dist))
  # sorted_sim_movies_dist = np.array(sim_movies_dist)[sorted_idx]
  # sorted_sim_movies_ind = np.array(sim_movies_ind)[sorted_idx]

  # sim_movies_dist = list(sorted_sim_movies_dist)[::-1]
  # sim_movies_ind = list(sorted_sim_movies_ind)[::-1]  

  movies=[]
  for i in sim_movies_ind:
    movies.append(df.index[i])
  print('The Nearest Movies to ',movie,' are: ' ,movies)
  print('The Distance from ',movie,' are: ', sim_movies_dist)
  print('Indices of Nearest Movies to ',movie,' are: ' ,sim_movies_ind)
  return(movies,sim_movies_dist,sim_movies_ind)

recom_mov,dist_colab,ind_colab = collab_filtering(colab_table1,'Gone Girl (2014)')

# recom_mov,dist_colab,ind_colab = collab_filtering(colab_table1,'X-Men (2000)')

#Less Distance means the movies are more similar

kk = np.array([1,2,3])
kp = kk.copy()
print(kp)

def predict_rating(zero_idx,df_nan,df_0,sim_movies_dist,sim_movies_ind):
  #Similarity = 1 - distance
  #S is movie_similarity
  S = [1-x for x in sim_movies_dist]
  # df_0 = np.array(df_0)
  # df_nan = np.array(df_nan)
  R = df_0.copy() #Has ratings

  for i in range(len(zero_idx)):
    row = zero_idx[i][0]
    col = zero_idx[i][1]
    #(row,col) is the iloc index of zero element
    #Value at df(row,col) indicates the rating of the movie 
    m = row
    u = col #u is also the index od the column
    
    numerator = 0
    denominator = 0
    for j in range(len(S)):
      R_j_u = R.iloc[j,u] 
      # print(R_j_u,S[j])
      # R_j_u = R[j,u] 
      #S[j] is S(m,j) i.e the similarity between movie m and movie j
      numerator += S[j]*R_j_u
      denominator +=S[j]

    #R_m_u is R(m,u) i.e the rating that will be used to update NaN values
    R_m_u = numerator/denominator

    df_nan.iloc[m,u] = R_m_u
    # print(R_m_u)
    # df_0[m,u] = R_m_u       
    # df_0 = pd.DataFrame(df_0)
    # df_nan = pd.DataFrame(df_nan)

colab_table_test = colab_table.copy(deep=True)

colab_table_test.isnull().sum().sum()

predict_rating(zero_indices,colab_table_test,colab_table1,dist_colab,ind_colab)

colab_table_test.isnull().sum().sum()

colab_table_test

import seaborn as sns

plt.figure(figsize=(10,10))
heatmap_collab_filt = sns.heatmap(data=colab_table_test)

ll = list(colab_table[2])
if(math.isnan(ll[0])):
  print(1)
# ll

def movie_recom_collab(userId,n_recommendations,df_filled,df_nan):
  movies_by_user = list(df_nan[userId])
  ratings=[]
  movies=[]
  for i in range(len(movies_by_user)):
    if(math.isnan(movies_by_user[i])):
      # data = np.array(df_filled)
      # col_names = list(df_filled.columns)
      # data = np.column_stack((data, col_names))
      # ratings.append(data[i, np.where(data[-1, :] == userId)])

      userId_idx = list(df_filled.columns).index(userId)
      ratings.append(df_filled.iloc[i,userId_idx])
      movies.append(df_filled.index[i])

  sorted_idx = np.argsort(np.array(ratings))
  sorted_ratings = np.array(ratings)[sorted_idx]
  sorted_movies = np.array(movies)[sorted_idx]

  sorted_ratings = sorted_ratings[::-1]
  sorted_movies = sorted_movies[::-1]

  print("The top recommendations are:-")
  for i in range(n_recommendations):
    print(str(i+1)+"."+str(sorted_movies[i]))
  return(sorted_movies[:n_recommendations])

recommendations_for_user = movie_recom_collab(15,5,colab_table_test,colab_table)

"""# Clustering"""

# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.sparse import csr_matrix
from mpl_toolkits.axes_grid1 import make_axes_locatable
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error
import itertools
from sklearn.metrics import silhouette_samples, silhouette_score

movies = df_movies.copy(deep=True)
ratings = df_ratings.copy(deep=True)[5000000:5000000+100000]





#Creating a function for getting genre rating for every genre in our dataset 
def get_genre_ratings(ratings, movies, genres, column_names):
    genre_ratings = pd.DataFrame()
    for genre in genres:        
        genre_movies = movies[movies['genres'].str.contains(genre) ]
        avg_genre_votes_per_user = ratings[ratings['movieId'].isin(genre_movies['movieId'])].loc[:, ['userId', 'rating']].groupby(['userId'])['rating'].mean().round(2)
        
        genre_ratings = pd.concat([genre_ratings, avg_genre_votes_per_user], axis=1)
        
    genre_ratings.columns = column_names
    return genre_ratings

genre_ratings = get_genre_ratings(ratings, movies, ['Romance', 'Sci-Fi'], ['avg_romance_rating', 'avg_scifi_rating'])
genre_ratings.head()

biased_dataset = genre_ratings
print( "Number of records: ", len(biased_dataset))
biased_dataset.head()

def draw_scatterplot(x_data, x_label, y_data, y_label):
    fig = plt.figure(figsize=(8,8))
    ax = fig.add_subplot(111)
    plt.xlim(0, 5)
    plt.ylim(0, 5)
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)
    ax.scatter(x_data, y_data, s=30)
# Plot the scatterplot
# draw_scatterplot(biased_dataset['avg_scifi_rating'],'Avg scifi rating', biased_dataset['avg_romance_rating'], 'Avg romance rating')

biased_dataset.fillna(0,inplace=True)
biased_dataset

#Creating a pivot table with userId's on x axis and movies on yaxis 
# This pivot table will help us in doing clustering 
ratings_title = pd.merge(ratings, movies[['movieId', 'title']], on='movieId' )
user_movie_ratings = pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')
# Print he number of dimensions and a subset of the dataset
print('dataset dimensions: ', user_movie_ratings.shape, '\n\nSubset example:')
user_movie_ratings

def get_most_rated_movies(user_movie_ratings, max_number_of_movies):
    
    user_movie_ratings = user_movie_ratings.append(user_movie_ratings.count(), ignore_index=True)
    user_movie_ratings_sorted = user_movie_ratings.sort_values(len(user_movie_ratings)-1, axis=1, ascending=False)
    user_movie_ratings_sorted = user_movie_ratings_sorted.drop(user_movie_ratings_sorted.tail(1).index)
    most_rated_movies = user_movie_ratings_sorted.iloc[:, :max_number_of_movies]
    return most_rated_movies

user_movie_ratings =  pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')
most_rated_movies_1k = get_most_rated_movies(user_movie_ratings, 1000)
most_rated_movies_1k

# sparse_ratings = csr_matrix(pd.SparseDataFrame(most_rated_movies_1k).to_coo())

filled_dataset = most_rated_movies_1k.fillna(0)

def draw_movies_heatmap(most_rated_movies_users_selection, axis_labels=True):
    
    fig = plt.figure(figsize=(15,4))
    ax = plt.gca()
    
    # Draw heatmap
    heatmap = ax.imshow(most_rated_movies_users_selection,  interpolation='nearest', vmin=0, vmax=5, aspect='auto')
    if axis_labels:
        ax.set_yticks(np.arange(most_rated_movies_users_selection.shape[0]) , minor=False)
        ax.set_xticks(np.arange(most_rated_movies_users_selection.shape[1]) , minor=False)
        ax.invert_yaxis()
        ax.xaxis.tick_top()
        labels = most_rated_movies_users_selection.columns.str[:40]
        ax.set_xticklabels(labels, minor=False)
        ax.set_yticklabels(most_rated_movies_users_selection.index, minor=False)
        plt.setp(ax.get_xticklabels(), rotation=90)
    else:
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    
    ax.grid(False)
    ax.set_ylabel('User id')
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.05)
    cbar = fig.colorbar(heatmap, ticks=[5, 4, 3, 2, 1, 0], cax=cax)
    cbar.ax.set_yticklabels(['5 stars', '4 stars','3 stars','2 stars','1 stars','0 stars'])
    plt.show()

#Creating clusters for every genre , and then visualizing clusters as well as  predicting further movies
predictions = KMeans(n_clusters=20, algorithm='full').fit_predict(filled_dataset)
max_users = 70
max_movies = 50
clustered = pd.concat([most_rated_movies_1k.reset_index(), pd.DataFrame({'group':predictions})], axis=1)

clustered

def sort_by_rating_density(user_movie_ratings, n_movies, n_users):
    most_rated_movies = get_most_rated_movies(user_movie_ratings, n_movies)
    # most_rated_movies = get_users_who_rate_the_most(most_rated_movies, n_users)
    return most_rated_movies
cluster_number = 11
n_users = 75
n_movies = 300
cluster = clustered[clustered.group == cluster_number].drop(['index', 'group'], axis=1)
cluster = sort_by_rating_density(cluster, n_movies, n_users)
draw_movies_heatmap(cluster, axis_labels=False)

cluster.fillna('').head()

cluster.iloc[19]

cluster.iloc[15]

#Predicting movies from the clusters we created before 
user_id = 15
user_2_ratings  = cluster.loc[user_id, :]
user_2_unrated_movies =  user_2_ratings[user_2_ratings.isnull()]
avg_ratings = pd.concat([user_2_unrated_movies, cluster.mean()], axis=1, join='inner').loc[:,0]
avg_ratings.sort_values(ascending=False)[:20]